{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxon IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxon_ids = [1493141]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number to download\n",
    "num_images = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_info(html_file):\n",
    "    with open(html_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    image_divs = soup.find_all(\"div\", class_=\"CoverImage low undefined loaded\")\n",
    "    \n",
    "    image_info = []\n",
    "    for div in image_divs:\n",
    "        style_attr = div.get(\"style\")\n",
    "        if style_attr:\n",
    "            url_start_index = style_attr.find(\"url(\") + len(\"url(\")\n",
    "            url_end_index = style_attr.find(\")\")\n",
    "            image_url = style_attr[url_start_index:url_end_index].strip('\"')\n",
    "            \n",
    "            # Extract image ID from URL\n",
    "            second_last_slash_index = image_url.rfind(\"/\", 0, image_url.rfind(\"/\"))  # Find the index of the second to last slash\n",
    "            image_id = image_url[second_last_slash_index + 1:image_url.rfind(\"/\")]  # Extract the substring between the second to last slash and the last slash\n",
    "\n",
    "            image_info.append((image_url, image_id))\n",
    "    \n",
    "    return image_info\n",
    "\n",
    "# Function to check if all images for a taxon exist in the folder\n",
    "def check_images_exist(image_info, taxon_id):\n",
    "    taxon_folder = f\"./data/{taxon_id}\"\n",
    "    if not os.path.exists(taxon_folder):\n",
    "        return False\n",
    "    \n",
    "    num_existing_images = len([file for file in os.listdir(taxon_folder) if file.endswith(\".jpg\")])\n",
    "    num_total_images = len(image_info)\n",
    "    \n",
    "    return num_existing_images == num_total_images\n",
    "\n",
    "# Function to download and save images\n",
    "def download_and_save_images(image_info, taxon_id, max_images=100):\n",
    "    print(f\"Downloading images for Taxon ID {taxon_id}. Total images available: {len(image_info)}\")\n",
    "    \n",
    "    taxon_folder = f\"./data/{taxon_id}\"\n",
    "    if not os.path.exists(taxon_folder):\n",
    "        os.makedirs(taxon_folder)\n",
    "    \n",
    "    downloaded_count = 0\n",
    "    for url, image_id in image_info:\n",
    "        if downloaded_count >= max_images:\n",
    "            print(f\"Reached the limit of {max_images} images for Taxon ID {taxon_id}.\")\n",
    "            break\n",
    "        \n",
    "        # Download and save the image if it doesn't exist already\n",
    "        image_path = f\"{taxon_folder}/{image_id}.jpg\"\n",
    "        if not os.path.exists(image_path):\n",
    "            with open(image_path, 'wb') as f:\n",
    "                f.write(requests.get(url).content)\n",
    "            downloaded_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download only images loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go to: https://www.inaturalist.org/taxa/{Taxon ID}. for example: \"https://www.inaturalist.org/taxa/1493141\"\n",
    "2. On the link next to the images where it says 'show more' click. A page with an image gallery opens. Scroll down, keep scrolling.\n",
    "3. When enough images have loaded, press F12. Copy the HTML element. \n",
    "4. Add the HTML content to a new file under data/websites.\n",
    "5. Name the HTML file the same as the taxon id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for Taxon ID 1493141. Total images available: 0\n"
     ]
    }
   ],
   "source": [
    "# Loop through each taxon id\n",
    "for taxon_id in taxon_ids:\n",
    "    # Construct the filename for the HTML file\n",
    "    html_file = f\"./data/websites/{taxon_id}.html\"\n",
    "    \n",
    "    # Check if the HTML file exists\n",
    "    if os.path.exists(html_file):\n",
    "        # Extract image URLs and image IDs from the HTML file\n",
    "        image_info = extract_image_info(html_file)\n",
    "        \n",
    "        # Check if all images for the taxon already exist\n",
    "        if check_images_exist(image_info, taxon_id):\n",
    "            print(f\"All images for Taxon ID {taxon_id} already exist. Skipping...\")\n",
    "        else:\n",
    "            # Download and save images\n",
    "            download_and_save_images(image_info, taxon_id, max_images=num_images)\n",
    "    else:\n",
    "        print(f\"HTML file for Taxon ID {taxon_id} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download images and meta data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go to: https://www.inaturalist.org/observations?place_id=any&subview=table&taxon_id={Taxon ID}. for example: \"https://www.inaturalist.org/observations?place_id=any&subview=table&taxon_id=1493141\"\n",
    "2. This is a list of observations of this taxon.Scroll down, and keep scrolling.\n",
    "3. When enough rows have loaded, press F12. Copy the HTML element. \n",
    "4. Add the HTML content to a new file under data/websites.\n",
    "5. Name the HTML file the same as the taxon id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_info(html_file, limit=10):\n",
    "    \"\"\"Extract image information from HTML file, limited to a specified number of images.\"\"\"\n",
    "    with open(html_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    observations = soup.find_all(\"tr\", class_=\"ng-scope\")\n",
    "    \n",
    "    image_info = []\n",
    "    for obs in observations[:limit]:  # Process only up to 'limit' rows\n",
    "        image_div = obs.find(\"a\", class_=\"img\")\n",
    "        if image_div:\n",
    "            style_attr = image_div.get(\"style\")\n",
    "            if style_attr and \"background-image\" in style_attr:\n",
    "                # Extract the URL directly from the style attribute\n",
    "                url_start_index = style_attr.find(\"url(\") + len(\"url(\")\n",
    "                url_end_index = style_attr.find(\")\", url_start_index)\n",
    "                image_url = style_attr[url_start_index:url_end_index].strip('\"\\'')\n",
    "\n",
    "                # Extract image ID from URL\n",
    "                image_id = image_url.split('/')[-2]\n",
    "                \n",
    "                date_span = obs.find(\"span\", class_=\"date ng-binding\")\n",
    "                time_span = obs.find(\"span\", class_=\"time ng-binding\")\n",
    "                place_td = obs.find(\"td\", class_=\"place ng-binding\")\n",
    "                \n",
    "                date = date_span.text.strip() if date_span else \"unknown\"\n",
    "                time = time_span.text.strip() if time_span else \"unknown\"\n",
    "                place = place_td.text.strip() if place_td else \"unknown\"\n",
    "                \n",
    "                image_info.append((image_url, image_id, date, time, place))\n",
    "                if len(image_info) >= limit:\n",
    "                    break\n",
    "    \n",
    "    return image_info\n",
    "\n",
    "def save_metadata_as_xml(image_info, taxon_id):\n",
    "    \"\"\"Save metadata as XML files.\"\"\"\n",
    "    for url, image_id, date, time, place in image_info:\n",
    "        taxon_folder = f\"./data/{taxon_id}\"\n",
    "        if not os.path.exists(taxon_folder):\n",
    "            os.makedirs(taxon_folder)\n",
    "        \n",
    "        # Create XML structure\n",
    "        image_metadata = ET.Element(\"ImageMetadata\")\n",
    "        ET.SubElement(image_metadata, \"ImageID\").text = image_id\n",
    "        ET.SubElement(image_metadata, \"URL\").text = url\n",
    "        ET.SubElement(image_metadata, \"Date\").text = date\n",
    "        ET.SubElement(image_metadata, \"Time\").text = time\n",
    "        ET.SubElement(image_metadata, \"Place\").text = place\n",
    "        \n",
    "        # Save XML to file\n",
    "        xml_str = ET.tostring(image_metadata, encoding=\"unicode\")\n",
    "        xml_path = os.path.join(taxon_folder, f\"{image_id}.xml\")\n",
    "        with open(xml_path, \"w\", encoding=\"utf-8\") as xml_file:\n",
    "            xml_file.write(xml_str)\n",
    "\n",
    "def download_and_save_images(image_info, taxon_id, max_images=100):\n",
    "    \"\"\"Download and save images.\"\"\"\n",
    "    print(f\"Downloading images for Taxon ID {taxon_id}. Total images downloaded: {len(image_info)}\")\n",
    "    \n",
    "    taxon_folder = f\"./data/{taxon_id}\"\n",
    "    if not os.path.exists(taxon_folder):\n",
    "        os.makedirs(taxon_folder)\n",
    "    \n",
    "    downloaded_count = 0\n",
    "    for url, image_id, _, _, _ in image_info:\n",
    "        if downloaded_count >= max_images:\n",
    "            print(f\"Reached the limit of {max_images} images for Taxon ID {taxon_id}.\")\n",
    "            break\n",
    "        \n",
    "        # Download and save the image if it doesn't exist already\n",
    "        image_path = f\"{taxon_folder}/{image_id}.jpg\"\n",
    "        if not os.path.exists(image_path):\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()  # Raise an error for bad status codes\n",
    "                with open(image_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                downloaded_count += 1\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Failed to download {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for Taxon ID 1493141. Total images downloaded: 10\n"
     ]
    }
   ],
   "source": [
    "for taxon_id in taxon_ids:\n",
    "    # Construct the filename for the HTML file\n",
    "    html_file = f\"./data/websites/{taxon_id}.html\"\n",
    "    \n",
    "    # Check if the HTML file exists\n",
    "    if os.path.exists(html_file):\n",
    "        # Extract image URLs and image IDs from the HTML file\n",
    "        image_info = extract_image_info(html_file, limit=num_images)\n",
    "        \n",
    "        if image_info:\n",
    "            # Save metadata as XML files\n",
    "            save_metadata_as_xml(image_info, taxon_id)\n",
    "\n",
    "            # Download and save images\n",
    "            download_and_save_images(image_info, taxon_id, max_images=num_images)\n",
    "        else:\n",
    "            print(f\"No images found for Taxon ID {taxon_id}.\")\n",
    "    else:\n",
    "        print(f\"HTML file for Taxon ID {taxon_id} does not exist.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
